{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_word2vec_continued.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxF77E9-4JwH",
        "colab_type": "code",
        "outputId": "52febb9e-d4c9-4d5f-ed72-225d56d1c130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "#@title <b><font color=\"red\">▶</font><font color=\"black\"> run this cell and restart runtime</font></b>\n",
        "\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=5fe6002725349c4d8750e355cfc6effe7d4b35052431669ec0d8a6935254f81d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gt38figl/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_QUdbx2pZD",
        "colab_type": "code",
        "outputId": "89f7d633-ad79-4950-ab0b-e01d448ad2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#@title <b><font color=\"red\">▶</font><font color=\"black\"> run this cell to prepare supplementary materials for the lesson</font></b>\n",
        "\n",
        "!rm -rf harbour-space-text-mining-course\n",
        "!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git\n",
        "import sys\n",
        "sys.path.append('harbour-space-text-mining-course')\n",
        "\n",
        "from tmcourse.utils import (\n",
        "    enable_mathjax_in_cell,\n",
        ")\n",
        "from tmcourse.node2vec import Node2VecGraph\n",
        "# from tmcourse.demo import (\n",
        "# )\n",
        "# from tmcourse.quiz import (\n",
        "# )\n",
        "from tmcourse.ipyquiz import Quiz\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install fasttext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'harbour-space-text-mining-course'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 475 (delta 104), reused 119 (delta 55), pack-reused 307\u001b[K\n",
            "Receiving objects: 100% (475/475), 40.72 MiB | 20.65 MiB/s, done.\n",
            "Resolving deltas: 100% (287/287), done.\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3020443 sha256=95c398380db6efbc7d4eccc1f7e081010d94a453b3640750cf0c2db2def167a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektuMJCkeeMt",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center><h1>More about word vectors</h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqjwDwQpd8ZM",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Plan for today\n",
        "1. GloVe\n",
        "1. node2vec\n",
        "1. FastText\n",
        "1. Word vectors in spaCy\n",
        "1. Coding session: build news summarizer with word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmRxDyahd67Y",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynuKhfewrHIQ",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "In word2vec, we predict co-occurrence probabilities $\\Pr(o|c)$ using dot product of vectors for $o$ and $c$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as_-SSjAtrBL",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Insight**: _ratios_ of co-occurrence probabilities also give information: they encode meaningful componens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuu_rwH2ul0h",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ratios-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WxL6Fo9uo6A",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/ratios-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN5StdvWuuJd",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "\n",
        "**Idea**: if we predict\n",
        "$$\n",
        "\\log\\Pr(o|c) = \\mathbf{v}_o \\cdot \\mathbf{v}_c\n",
        "$$\n",
        "then _vector differences_ correspond to ratios of co-oocurrence probabilities!\n",
        "$$\n",
        "\\log\\dfrac{\\Pr(o|c_2)}{\\Pr(o|c_1)} = \\mathbf{v}_o \\cdot (\\mathbf{v}_{c_2} - \\mathbf{v}_{c_1})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILXZDQ2Nvh3M",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "\n",
        "So use the following objective function (GloVe):\n",
        "$$\n",
        "L(\\theta) = \\sum_{i \\in V, j \\in V, i \\neq j} f(X_{ij}) \\cdot (\\mathbf{v}_i \\cdot \\tilde{\\mathbf{v}}_j + b_i + \\tilde{b}_j - X_{ij})^2\n",
        "$$\n",
        "where \n",
        "- $i, j$ -- words\n",
        "- $X_{ij}$ -- how many times words $i$ and $j$ co-occur\n",
        "- $\\mathbf{v}, \\tilde{\\mathbf{v}}, b, \\tilde{b}$ -- parameters\n",
        "- $f(X_{ij})$ -- suppress rare (noisy) counters\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/f.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m91930Kfx0Co",
        "colab_type": "code",
        "outputId": "2f331447-4f71-4f0b-8ac8-717d4d3b4627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "#@slideshow slide tags=remove_input\n",
        "import gensim.downloader as api\n",
        "\n",
        "# list all available models\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "all_gensim_models = api.info()[\"models\"]\n",
        "gensim_model_description = [(m, all_gensim_models[m][\"description\"]) for m in all_gensim_models]\n",
        "display(HTML(tabulate(gensim_model_description, headers=(\"model name\", \"description\"), tablefmt=\"html\")))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>model name                        </th><th>description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>fasttext-wiki-news-subwords-300   </td><td>1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </td></tr>\n",
              "<tr><td>conceptnet-numberbatch-17-06-300  </td><td>ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.</td></tr>\n",
              "<tr><td>word2vec-ruscorpora-300           </td><td>Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </td></tr>\n",
              "<tr><td>word2vec-google-news-300          </td><td>Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in &#x27;Distributed Representations of Words and Phrases and their Compositionality&#x27; (https://code.google.com/archive/p/word2vec/).                                                                                                                                                                                                                              </td></tr>\n",
              "<tr><td>glove-wiki-gigaword-50            </td><td>Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
              "<tr><td>glove-wiki-gigaword-100           </td><td>Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
              "<tr><td>glove-wiki-gigaword-200           </td><td>Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
              "<tr><td>glove-wiki-gigaword-300           </td><td>Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
              "<tr><td>glove-twitter-25                  </td><td>Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
              "<tr><td>glove-twitter-50                  </td><td>Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
              "<tr><td>glove-twitter-100                 </td><td>Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
              "<tr><td>glove-twitter-200                 </td><td>Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
              "<tr><td>__testing_word2vec-matrix-synopsis</td><td>[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxyyyyPcX9Uu",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# node2vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkP2-BOf3GO",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "word2vec algorithm is actually not specific for texts: it is applicable for any _discrete sequences_ (if the total number of elements is finite)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxyCMwtzhJGh",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "Can we apply it for nodes in graphs?\n",
        "- Find similar nodes.\n",
        "- Extract features for machine learning.\n",
        "\n",
        "![alt text](https://snap.stanford.edu/node2vec/homo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx3qRTPKgtmD",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Idea**: graph traversal (BFS, DFS, ...) returns a sequence of nodes - what if we apply word2vec?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lvQYe9dheHk",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "node2vec algorithm:\n",
        "- For each vertex, generate $n$ random walks of length $k$ each\n",
        "- Parametrize random walks to make it \"BFS-like\" or \"DFS-like\".\n",
        "\n",
        "Example of transition from $t$ to $v$: $x_1$ is the neighbor of $t$, $x_2$ and $x_3$ are not.\n",
        "\n",
        "![alt text](https://snap.stanford.edu/node2vec/walk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMATDiL8fCBu",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Colab demo: find similar GoT characters with node2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-NsMdSTAHRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "903a18e0-4ebc-4cce-82ff-ab2ad8394a02"
      },
      "source": [
        "# read more about the data: https://www.kaggle.com/moradnejad/interaction-networks-for-game-of-thrones-saga\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"harbour-space-text-mining-course/datasets/got/asoiaf-book5-edges.csv\")\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "      <th>Type</th>\n",
              "      <th>weight</th>\n",
              "      <th>book</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aegon-I-Targaryen</td>\n",
              "      <td>Daenerys-Targaryen</td>\n",
              "      <td>undirected</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aegon-Targaryen-(son-of-Rhaegar)</td>\n",
              "      <td>Daenerys-Targaryen</td>\n",
              "      <td>undirected</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aegon-Targaryen-(son-of-Rhaegar)</td>\n",
              "      <td>Elia-Martell</td>\n",
              "      <td>undirected</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon-Targaryen-(son-of-Rhaegar)</td>\n",
              "      <td>Franklyn-Flowers</td>\n",
              "      <td>undirected</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aegon-Targaryen-(son-of-Rhaegar)</td>\n",
              "      <td>Haldon</td>\n",
              "      <td>undirected</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Source              Target  ... weight  book\n",
              "0                 Aegon-I-Targaryen  Daenerys-Targaryen  ...      4     5\n",
              "1  Aegon-Targaryen-(son-of-Rhaegar)  Daenerys-Targaryen  ...     11     5\n",
              "2  Aegon-Targaryen-(son-of-Rhaegar)        Elia-Martell  ...      4     5\n",
              "3  Aegon-Targaryen-(son-of-Rhaegar)    Franklyn-Flowers  ...      3     5\n",
              "4  Aegon-Targaryen-(son-of-Rhaegar)              Haldon  ...     14     5\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyTfA3e5CTeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a1c1ab15-ec09-4a10-e35c-6e89612e8f40"
      },
      "source": [
        "# create the weighted graph\n",
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "for i, r in df.iterrows():\n",
        "    G.add_edge(r[\"Source\"], r[\"Target\"], weight=r[\"weight\"])\n",
        "\n",
        "# preprocess the graph and generate random walks\n",
        "node2vec_G = Node2VecGraph(G, p=1, q=0.01)\n",
        "walks = node2vec_G.simulate_walks(num_walks=30, walk_length=10)\n",
        "print(\"Total walks:\", len(walks))\n",
        "from pprint import pprint\n",
        "pprint(walks[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total walks: 9510\n",
            "['Yezzan-zo-Qaggaz',\n",
            " 'Scar',\n",
            " 'Tyrion-Lannister',\n",
            " 'Nurse',\n",
            " 'Yezzan-zo-Qaggaz',\n",
            " 'Scar',\n",
            " 'Tyrion-Lannister',\n",
            " 'Nurse',\n",
            " 'Yezzan-zo-Qaggaz',\n",
            " 'Sweets']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJW4c84BD-VR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "77bc0614-1b54-4abc-8662-a4eaa98c0cf9"
      },
      "source": [
        "# train word2vec on walks\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "gensim_word2vec = Word2Vec(\n",
        "    sg=1,  # skip-gram\n",
        "    hs=0,  # negative sampling\n",
        "    size=200,\n",
        "    window=3,\n",
        "    alpha=0.01,\n",
        "    negative=20\n",
        ")\n",
        "gensim_word2vec.build_vocab(walks)\n",
        "gensim_word2vec.train(walks, total_examples=len(walks), epochs=10)\n",
        "# find characters similar to Jon Show\n",
        "pprint(gensim_word2vec.wv.most_similar(\"Jon-Snow\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Ulmer', 0.9032264947891235),\n",
            " ('Alliser-Thorne', 0.8938930034637451),\n",
            " ('Tom-Barleycorn', 0.8850666880607605),\n",
            " ('Benjen-Stark', 0.865203857421875),\n",
            " ('Soren-Shieldbreaker', 0.8634392023086548),\n",
            " ('Ygritte', 0.8586257696151733),\n",
            " ('Sigorn', 0.8561915755271912),\n",
            " ('Torghen-Flint', 0.8504267930984497),\n",
            " ('Melisandre', 0.836707592010498),\n",
            " ('Brandon-Norrey', 0.8366614580154419)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc3nZX4LGTm3",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQVI4K2oPYfl",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Word vectors with subword information\n",
        "\n",
        "Sometimes we may guess (at least partially) the meaning of a word by looking at its pieces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK3dNVBbjsiS",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "For example, for the word \"athazagoraphobia\" we may guess it is a phobia (a fear of something)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgrtCdmWj8oK",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "(In case you are wondering, \"athazagoraphobia\" is the fear of being forgotten or ignored and fear of forgetting.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LDevDPHkMuv",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Idea**: learn not only word vectors, but also subword vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tii6l_oYjrkm",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Algorithm**:\n",
        "- Represent each word as a collection of character $n$-gram.\n",
        "- Include the word w itself in the set of its $n$-grams.\n",
        "- Add special boundary symbols `<` and `>` at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences.\n",
        "- Word vector is the sum of the vector representations of its $n$-grams.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MSRxWf5lI-M",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Example**: for $n=3$, the word \"where\" is represented as\n",
        "\n",
        "  `<wh, whe, her, ere, re>, <where>`\n",
        "    \n",
        "> Note that the sequence `<her>`, corresponding to the word \"her\" is different from the trigram `her` from the word \"where\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfE6njjJlP7_",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Colab demo: FastText (unsupervised)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRfiTnvFGV5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmqrHVV9QJ_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(subset=\"all\", shuffle=True, random_state=1, categories=(\"sci.space\",), remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAtyCKn6VeRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_text(t):\n",
        "    import re\n",
        "    s = \" \".join(t.split()).lower()  # lowercase\n",
        "    return re.sub(r'[^\\w\\s]',' ',s)  # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI66Gdq9VlrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"texts.txt\", \"w\") as f:\n",
        "    for t in dataset.data:\n",
        "        f.write(prepare_text(t) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ7nZAA-WIdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext.train_unsupervised(\n",
        "    input=\"texts.txt\",\n",
        "    epoch=10\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzrHX3UMWXns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b95e9334-b165-475f-e0af-68062effd6a0"
      },
      "source": [
        "model.get_nearest_neighbors(\"satellite\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9310543537139893, 'satellites'),\n",
              " (0.826610267162323, 'sat'),\n",
              " (0.7046252489089966, 'radar'),\n",
              " (0.684397280216217, 'iridium'),\n",
              " (0.6659807562828064, 'tracking'),\n",
              " (0.6609216928482056, 'telecommunications'),\n",
              " (0.6603425145149231, 'korean'),\n",
              " (0.656222939491272, 'intelsat'),\n",
              " (0.6515015959739685, 'european'),\n",
              " (0.6475222706794739, 'japan')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kifiPUVZuH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1783c33b-91ef-45ef-94c3-89dc73c19d34"
      },
      "source": [
        "model.get_nearest_neighbors(\"satelite\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.965173065662384, 'satellite'),\n",
              " (0.8960800766944885, 'satellites'),\n",
              " (0.8491849303245544, 'sat'),\n",
              " (0.7472147345542908, 'items'),\n",
              " (0.7320234775543213, 'korean'),\n",
              " (0.723210334777832, 'radar'),\n",
              " (0.720881462097168, 'european'),\n",
              " (0.7101620435714722, 'iridium'),\n",
              " (0.709301233291626, 'olympus'),\n",
              " (0.7044820785522461, 'intelsat')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWkAji_9JvyT",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Text classification\n",
        "\n",
        "**Idea**: \n",
        "- Represent a text as the average of word vectors.\n",
        "- Learn matrix $A$ such that $\\mathrm{softmax}(A \\cdot v)$ gives probability distribution over classes.\n",
        "- Instead of the loss function for word2vec (negative log-likelihood), optimize the loss function for classification (cross-entropy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkoecYhJm84p",
        "colab_type": "code",
        "outputId": "ed2c2d0d-c3c8-4e69-e81c-4a61fec497f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "#@slideshow fragment tags=remove_input\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
        "\n",
        "x = [-0.1, -1, 0.7, 0.5, 0.33]\n",
        "i = list(range(len(x)))\n",
        "ax1.bar(i, x)\n",
        "ax2.bar(i, softmax(x))\n",
        "ax1.set_title(r\"$X = {}$\".format(x))\n",
        "ax2.set_title(r\"$\\mathrm{softmax}(X) \\sim e^X$\")\n",
        "ax1.set_xticks([])\n",
        "ax2.set_xticks([])\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6AFnFxElWad",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Colab demo: FastText (supervised)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogVXd469JxYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "fetch_params = dict(\n",
        "    shuffle=True,\n",
        "    random_state=1,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "train_dataset = fetch_20newsgroups(subset=\"train\", **fetch_params)\n",
        "test_dataset = fetch_20newsgroups(subset=\"test\", **fetch_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_EKqGHMKaUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_dataset_to_fasttext_file(texts, labels, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        for t, l in zip(texts, labels):\n",
        "            t = prepare_text(t)\n",
        "            # labels are stored in file with prefix \"__label__\"\n",
        "            f.write(f\"__label__{l}\\t{t}\\n\")\n",
        "\n",
        "convert_dataset_to_fasttext_file(\n",
        "    train_dataset.data,\n",
        "    [train_dataset.target_names[c] for c in train_dataset.target],\n",
        "    \"train.txt\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjfBYcpAU2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "856a6f00-d505-405b-8463-f1cb65964919"
      },
      "source": [
        "# in ipython use % for certain cmds like %cd /path/\n",
        "!head -n1 train.txt"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__label__talk.politics.mideast\twell i m not sure about the story nad it did seem biased  what i disagree with is your statement that the u s  media is out to ruin israels reputation  that is rediculous  the u s  media is the most pro israeli media in the world  having lived in europe i realize that incidences such as the one described in the letter have occured  the u s  media as a whole seem to try to ignore them  the u s  is subsidizing israels existance and the europeans are not  at least not to the same degree   so i think that might be a reason they report more clearly on the atrocities  what is a shame is that in austria  daily reports of the inhuman acts commited by israeli soldiers and the blessing received from the government makes some of the holocaust guilt go away  after all  look how the jews are treating other races when they got power  it is unfortunate \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5i8yuN2KyKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext.train_supervised(\n",
        "    input=\"train.txt\",\n",
        "    lr=0.1,\n",
        "    epoch=25\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgXGm6t-Lz_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7982d15-4bc3-42b8-e537-7f3c107b8412"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "for t, c in zip(test_dataset.data, test_dataset.target):\n",
        "    total += 1\n",
        "    label = test_dataset.target_names[c]\n",
        "    predicted_label = model.predict(prepare_text(t))[0][0][len(\"__label__\"):]\n",
        "    if label == predicted_label:\n",
        "        correct += 1\n",
        "print(f\"accuracy: {correct / total}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.5762081784386617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UufFB2dq1WHx",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Colab demo: word vectors in spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgdJ4HbQ1uN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "# spaCy is equipped by pre-trained word vectors\n",
        "# for example, they are stored in \"en_core_web_lg\"\n",
        "# in Colab, you need to run !python -m spacy download en_core_web_lg (the first cell) and restart runtime\n",
        "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"ner\", \"parser\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "157ScdZJ5W98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "99b99bc6-dbae-4f99-f921-eb44a60c4a7d"
      },
      "source": [
        "doc = nlp(\"dog cat banana afskfsd\")\n",
        "\n",
        "# you can check the attribute `.has_vector` of a token\n",
        "for token in doc:\n",
        "    print(f\"Token '{token.text}' has vector: {token.has_vector}\")\n",
        "\n",
        "# doc (parsed text, the collection of tokens) also has vector (the average over all tokens)\n",
        "print(f\"Doc has vector: {doc.has_vector}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token 'dog' has vector: True\n",
            "Token 'cat' has vector: True\n",
            "Token 'banana' has vector: True\n",
            "Token 'afskfsd' has vector: False\n",
            "Doc has vector: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeHqgw6H4leg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38764c5d-23a0-4fbd-de05-311f91b6ef0c"
      },
      "source": [
        "# for any pair of objects that have vectors, we can find similarity\n",
        "# the example from the last lesson\n",
        "doc1 = nlp(\"How can I be a good geologist?\")\n",
        "doc2 = nlp(\"What should I do to be a great geologist?\")\n",
        "doc3 = nlp(\"What should I do to be a good geologist?\")\n",
        "doc1.similarity(doc2), doc1.similarity(doc3)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9807831604694341, 0.9820952641696503)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7qKHKpo4vHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in particular, any subsequence of a document (so-called Span) also may have a vector\n",
        "# for example, a sentence in a document has vector (the average over all tokens of the sentence)\n",
        "# to split the document into sentences, we need to add 'sentencizer' to NLP pipeline\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85tR-5X_5hKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "c2dff24c-4d19-4947-c4ce-ab904ba50fe9"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(\n",
        "    subset=\"all\",\n",
        "    shuffle=True,\n",
        "    random_state=1,\n",
        "    categories=(\"sci.space\",),\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "from pprint import pprint\n",
        "\n",
        "# after the sentenciser has been added, the parsed document contains `.sents` attribute:\n",
        "for sentence in nlp(dataset.data[0]).sents:\n",
        "    print(\"SENTENCE:\\n\", sentence)\n",
        "    print(\"HAS VECTOR:\", sentence.has_vector)\n",
        "    print(\"---\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SENTENCE:\n",
            " Re: Space billboards\n",
            "\n",
            "Even easier to implement than writing messages on the Moon, once upon\n",
            "a time a group of space activists I belonged to in Seattle considered\n",
            "a \"Goodyear Blimp in orbit\".\n",
            "HAS VECTOR: True\n",
            "---\n",
            "SENTENCE:\n",
            "  The idea was to use a large structure\n",
            "that could carry an array of lights like the Goodyear Blimp has.\n",
            "HAS VECTOR: True\n",
            "---\n",
            "SENTENCE:\n",
            " \n",
            "Placed in a low Earth orbit of high inclination, it could eventually\n",
            "be seen by almost everyone on Earth.\n",
            "HAS VECTOR: True\n",
            "---\n",
            "SENTENCE:\n",
            "  Only our collective disapproval\n",
            "of cluttering up space with such a thing stopped us from pursuing\n",
            "it.\n",
            "HAS VECTOR: True\n",
            "---\n",
            "SENTENCE:\n",
            "  It had quite feasible economics, which I will not post here\n",
            "because I don't want to encourage the idea (if you want to do such\n",
            "a thing, go figure it out for yourself).\n",
            "HAS VECTOR: True\n",
            "---\n",
            "SENTENCE:\n",
            " \n",
            "\n",
            "Dani Eder\n",
            "\n",
            "HAS VECTOR: True\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMd0u7O0GD8",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Coding session\n",
        "\n",
        "Build news summarizer using word2vec.\n",
        "\n",
        "Try the following idea:\n",
        "1. Assume that there is a \"central\" vector (the main idea) of the article which is the average vector of all tokens in the article.\n",
        "1. Find $k$ sentences closest to the \"central\" vector. The assumption is that these sentences are \"informative\", and other sentences are \"noisy\" and not important.\n",
        "\n",
        "The function `parse_techcrunch_url(url)` parses the content from [techcrunch.com](https://techcrunch.com), you can use it to check your implementation on real data.\n",
        "\n",
        "Also, implement a random baseline (get $k$ random sentences from the text) and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAzNZd420m_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ede844f-1007-400f-b2af-4769275b471c"
      },
      "source": [
        "def parse_techcrunch_url(url):\n",
        "    import bs4\n",
        "    import re\n",
        "    import requests\n",
        "    response = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
        "    items = soup.find(\"div\", {\"class\": \"article-content\"}).findAll(\"p\")\n",
        "    raw_html = \"\\n\".join(map(str, items))\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    clean_html = re.sub(cleanr, '', raw_html)\n",
        "    return clean_html\n",
        "\n",
        "from pprint import pprint\n",
        "url = \"https://techcrunch.com/2020/05/23/hackers-iphone-new-jailbreak/\"\n",
        "parsed_url = parse_techcrunch_url(url)\n",
        "print(type(parsed_url))\n",
        "# pprint(parsed_url)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOwd_J_4JjoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_text(text, k: int):\n",
        "    # YOUR CODE HERE\n",
        "  import spacy, operator\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"ner\", \"parser\"])\n",
        "  nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
        "  doc = nlp(text)\n",
        "  vec_rankings = []\n",
        "  i = 0\n",
        "  for sentence in doc.sents:\n",
        "    sent_rank = doc.similarity(sentence)\n",
        "    vec_rankings.append((i, sent_rank, sentence))\n",
        "    i += 1\n",
        "  return sorted(vec_rankings, key=operator.itemgetter(1, 0), reverse=True)[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27L3k3bnZjN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "609b93c3-c793-4a35-9f49-c8651a602986"
      },
      "source": [
        "pprint(summarize_text(parsed_url, 2)) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1,\n",
            "  0.9795232646837688,\n",
            "  \n",
            "For as long as Apple  has kept up its “walled garden” approach to iPhones by only allowing apps and customizations that it approves, hackers have tried to break free from what they call the “jail,” hence the name “jailbreak.”),\n",
            " (6,\n",
            "  0.9744662879431004,\n",
            "  \n",
            "Details of the vulnerability that the hackers used to build the jailbreak aren’t known, but it’s not expected to last forever.)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clMlNxjE5If4",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Recommended resources\n",
        "- [node2vec](https://snap.stanford.edu/node2vec/)\n",
        "- FastText\n",
        "  - [Documentation](https://fasttext.cc/)\n",
        "  - [Paper \"Enriching Word Vectors with Subword Information\"](https://arxiv.org/abs/1607.04606)\n",
        "  - [Paper \"Bag of Tricks for Efficient Text Classification\"](https://arxiv.org/abs/1607.01759)\n",
        "- [vectors in spaCy](https://spacy.io/usage/vectors-similarity)"
      ]
    }
  ]
}