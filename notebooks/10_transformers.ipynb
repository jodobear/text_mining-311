{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 10_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGpGrCsW_PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_VB3jivlhR",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center><h1>Transformers</h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFenQj7SVDaT",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/gpt-3.png\">\n",
        "\n",
        "<a href=\"https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd\">link</a>\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lgEgLI_axn",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Outline\n",
        "1. Sequence-to-Sequence (seq2seq) learning. Encoder-decoder architecture.\n",
        "1. Motivation of Transformer.\n",
        "1. Attention\n",
        "1. Transformer block\n",
        "1. Encoder\n",
        "1. Decoder\n",
        "1. BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtPSBec6vurK",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# seq2seq. Encoder-decoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZWREaePv43B",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "RNN for classification\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-classification.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC-qT8p1y6pL",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "RNN for language modeling\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-lm.png\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmd_I63sy79T",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "In the demo [language model with RNN](https://colab.research.google.com/drive/1gg6RZqjz6d-1o6pguZaktwHUVAFyScoX#scrollTo=YCjdUfGrsqWr&line=11&uniqifier=1) we trained a <font color=\"red\">conditioned</font> RNN:\n",
        "\n",
        "$$\n",
        "h_t = f(h_{t-1}, x_t, \\color{red}{c})\n",
        "$$\n",
        "$$\n",
        "y_t = g(h_t)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RElswzPizD6J",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "We need conditioning in many applications:\n",
        "- Machine translation (English $\\rightarrow$ Spanish)\n",
        "- Text summarization (long text $\\rightarrow$ short text)\n",
        "- Dialogue / Chatbots / Question answering\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZHvRfs1y9fg",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Idea**:\n",
        "1. _Learn_ the condition from the first sequence (many-to-one RNN).\n",
        "1. Use the learned condition to generate the second sequence (many-to-many language model RNN).\n",
        "\n",
        "> This is called sequence-to-sequence (seq2seq) learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcg_7ZUEy-7T",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "RNN for seq2seq\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/rnn-seq2seq.png\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox2CGd8R1Gjc",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "Definitions:\n",
        "- The part that _learns_ the condition is called **encoder**.\n",
        "- The part that generates the result based on the condition is called **decoder**.\n",
        "\n",
        "**Intuition**:\n",
        "- Encoder learns (\"encodes\") the representation of the input sequence\n",
        "- Decoder reconstructs (\"decodes\") the text from the learned representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re4T5RtN1q-l",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Motivation of Transformer\n",
        "\n",
        "**Problem**: RNN is sequential (= slow).\n",
        "\n",
        "**Q**: But _why_ do we use it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Blq_nC92Eg0",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**A**: RNN accepts texts of any length and should learn _long-term dependencies_ between words (at least, in theory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZzbUuo24Ga",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "This is how dependencies may look like:\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/word-dependencies.png\" style=\"width:70%;\">\n",
        "<table>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td></td>\n",
        "    <td><b>This</b></td>\n",
        "    <td><b>is</b></td>\n",
        "    <td><b>a</b></td>\n",
        "    <td><b>sentence</b></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>This</b></td>\n",
        "    <td>DET</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>is</b></td>\n",
        "    <td>nsubj</td>\n",
        "    <td>AUX</td>\n",
        "    <td></td>\n",
        "    <td>attr</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>a</b></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>DET</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><b>sentence</b></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>det</td>\n",
        "    <td>NOUN</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvF9UvE6mIF",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOKdC6pCD21K",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "RNNs produce **word vectors** based on **dependencies** between words.\n",
        "\n",
        "**Idea**: represent words as vectors and measure dependency using dot product.\n",
        "\n",
        "**Our goal**: represent the word $q$ given the sequence $k_1, k_2, \\dots$\n",
        "\n",
        "> $q$ does not necessarily belong to the sequence!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnXo2ijjD4Wg",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Definitions**:\n",
        "- $k_i$ is a **key** vector of size $d_k$.\n",
        "- Each key $k_i$ has the associated **value** - vector $v_i$ of size $d_v$.\n",
        "- $q$ is a **query** vector of size $\\color{red}{d_k}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZyEkhsXEEuU",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "- Dependency between $q$ and $k_i$ is quantified by a dot product:\n",
        "$$\n",
        "\\mathrm{softmax}(q \\cdot k_i) = \\dfrac{\\exp(q \\cdot k_i)}{\\sum_j \\exp(q \\cdot k_j)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb74kmsnEFyK",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "- The vector for $q$ is the average value vector:\n",
        "$$\n",
        "A(q, k_1, v_1, k_2, v_2, \\dots) = \\sum_i v_i \\cdot \\mathrm{softmax}(q \\cdot k_i)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdhytGjSEGzS",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "Put all keys into matrix $K$ and all values into matrix $V$:\n",
        "$$\n",
        "A(q, K, V) = \\sum_i v_i \\cdot \\mathrm{softmax}(q \\cdot k_i)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvzxNcK3EH5v",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "If we have many queries, put them into matrix $Q$:\n",
        "$$\n",
        "A(Q, K, V) = \\mathrm{softmax}(Q K^T) V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gorCCa68Ebfw",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Problem**: for large $d_k$, dot products are large and softmax is \"peaked\".\n",
        "\n",
        "**Solution**: normalize the dot product:\n",
        "$$\n",
        "A(Q, K, V) = \\mathrm{softmax}\\left(\\dfrac{Q K^T}{\\color{red}{\\sqrt{d_k}}}\\right) V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8SyPQmlJREh",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi-0C2n1FHuP",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "The function $A(Q, K, V)$ is called **Dot-Product Attention** function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3ARyhd1HjVx",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "In the **encoder**: $Q = K = V$.\n",
        "\n",
        "> In other words: the word vectors themselves select each other\n",
        "\n",
        "We’ll see in the **decoder** why we separate them in the definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm4XLKrhIPHl",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Problem**: only one way for words to interact with one-another.\n",
        "\n",
        "**Solution**: _multi-head_ attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlaFuweWIiKv",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "- First map $Q, K, V$ into $h=8$ lower dimensional spaces via linear layers.\n",
        "- Then apply attention.\n",
        "- Then concatenate outputs and pipe through linear layer.\n",
        "\n",
        "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGrX4TwgJqEa",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Transformer block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAz1uKtuK6O6",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Transformer block** is a layer that has two “sublayers”\n",
        "1. Multihead attention\n",
        "2. 2-layer feed-forward neural network (with ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8N8E5emKuKq",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/transformer-block.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyOIGAeLGh_",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "\n",
        "Techniques to speed up training:\n",
        "- Residual connection\n",
        "- Dropout\n",
        "- Layer normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X4LGwyjMSME",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Residual connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jkquu8NAOb",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "For a layer $\\mathcal{F}(\\mathbf{x})$, the _residual connection_ is $\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/residual.JPEG\" style=\"width:50%\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR2qTbRLNmOo",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Idea**: it helps to propagate the gradient\n",
        "$$\n",
        "\\dfrac{\\partial (\\mathcal{F}(\\mathbf{x}) + \\mathbf{x})}{\\partial \\mathbf{x}} = \\dfrac{\\partial \\mathcal{F}(\\mathbf{x})}{\\partial \\mathbf{x}} \\color{red}{+ 1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHCaxtixMQ6e",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Dropout\n",
        "Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training.\n",
        "\n",
        "![dropout](https://raw.githubusercontent.com/horoshenkih/harbour-space-ds210/master/pic/dropout.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w2d0PBkRMNh",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Intuition**: voting over $2^N$ thinned networks with shared weights\n",
        "\n",
        "![thinned networks](https://raw.githubusercontent.com/horoshenkih/harbour-space-ds210/master/pic/thinned-networks.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iddrKdWtMT_g",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOIEwyEMWwbS",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "\n",
        "Layer normalization changes input to have mean 0 and variance 1, per layer and per training point (and adds two more parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3xmXcrvW78Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@slideshow fragment\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2dXiwfpXRlP",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "\n",
        "From the [original paper](https://arxiv.org/pdf/1607.06450.pdf):\n",
        "\n",
        "> Layer normalization is very effective at **stabilizing** the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially **reduce the training time** compared with previously published techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7i2EjO6PyiT",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "Back to the **transformer block**: 2 sublayers\n",
        "1. Multihead attention\n",
        "2. 2-layer feed-forward neural network (with ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hn3u8gmMYth",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "To speed up training, the actual output of each $\\mathrm{Sublayer}(x)$ is\n",
        "$$\n",
        "\\mathrm{LayerNorm}(x + \\mathrm{Dropout}(\\mathrm{Sublayer}(x)))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16g0rv-ebmSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@slideshow fragment\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oK3fDzYIwH",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjAb03XrYN5H",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Input: byte-pair encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzSkDnmwZi5M",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Problem**: represent rare words (like \"athazagoraphobia\").\n",
        "\n",
        "**Solution**: use subword units.\n",
        "> In FastText, $n$-grams are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLBAhc9OZ8qX",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Q**: which $n$ shall we use?\n",
        "\n",
        "**A**: don't fix $n$, extract frequent subwords instead.\n",
        "\n",
        "> This is inspired by the data compression technique called Byte Pair Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8sLAdE-ZWRM",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "![](https://miro.medium.com/max/1400/1*x1Y_n3sXGygUPSdfXTm9pQ.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsqG9zQTaldY",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "In Tex Mining, BPE is slightly modified in its implementation: the frequently occurring subword pairs are **merged together instead of being replaced** by another byte to enable compression.\n",
        "\n",
        "> This would basically lead the rare word `athazagoraphobia` to be split up into more frequent subwords such as `['▁ath', 'az', 'agor', 'aphobia']`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfqUDkyla2Lr",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "1. Initialize vocabulary.\n",
        "2. Represent each word in the corpus as a combination of the characters along with the special end of word token `</w>`.\n",
        "2. Iteratively count character pairs in all tokens of the vocabulary.\n",
        "4. Merge every occurrence of the most frequent pair, add the new character n-gram to the vocabulary.\n",
        "5. Repeat step 4 until the desired number of merge operations are completed or the desired vocabulary size is achieved (which is a hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WKvaCjyYQLE",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Input: positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqTcbvRb3KK",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Problem**: we don't want the model to be sequential, but word order is important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4bI_vpmcoNd",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Solution**: represent the _position_ as a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WLaopnidF58",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "The exact formula:\n",
        "$$\n",
        "PE(pos) = \\begin{pmatrix}\n",
        "\\cos\\left(\\dfrac{pos}{1}\\right) \\\\\n",
        "\\sin\\left(\\dfrac{pos}{1}\\right) \\\\\n",
        "\\cos\\left(\\dfrac{pos}{10000^{\\frac{2}{d}}}\\right) \\\\\n",
        "\\sin\\left(\\dfrac{pos}{10000^{\\frac{2}{d}}}\\right) \\\\\n",
        "\\dots \\\\\n",
        "\\cos\\left(\\dfrac{pos}{10000}\\right) \\\\\n",
        "\\sin\\left(\\dfrac{pos}{10000}\\right) \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "From the [original paper](https://arxiv.org/pdf/1706.03762.pdf):\n",
        "> it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE(pos+k)$ can be represented as a linear function of $PE(pos)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUtqQjiYUBM",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Complete encoder\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/transformer-encoder.png\">\n",
        "</center>\n",
        "Blocks are repeated 6 times (in vertical stack)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPihF4sCg2Kc",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "![](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/attention-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWpSP4Slg5V-",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "![](https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/attention-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0xecLUBg8Hc",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyW7C4aKhaNe",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Masked self-attention\n",
        "  $$\n",
        "    A(q, k_1, v_1, k_2, v_2, \\dots) = \\sum_{i \\color{red}{ < t}} v_i \\cdot \\mathrm{softmax}(q \\cdot k_i)\n",
        "  $$\n",
        "  where $q = k_t$ for some $t$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f4b46QVlQZE",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "Why do we need masking?\n",
        "- Suppose $k_t$ attends to some future word $k_{t + \\delta}$.\n",
        "- The word $k_{t + \\delta}$ may attend the word $k_t$ **in one of the previous transformer blocks**.\n",
        "- So $k_t$ may effectively \"see itself\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEid8amPio40",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "## Encoder-Decoder Attention\n",
        "- queries come from previous decoder layer\n",
        "- keys and values come from output of encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvT4yqPejToS",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "<img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3aOoEuejhGq",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# BERT (Bidirectional Encoder Representations from Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QiLLXPjmW78",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "**Problem**: Transformer uses _masking_ so that words cannot \"see themselves\" - but _language understanding_ is bidirectional!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm-NtfofmzQh",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "Remember this example?\n",
        "1. \"Students opened their ___ as the proctor started the clock.\"\n",
        "1. \"Students opened their ___ and started coding.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXzsBeoXm_IM",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow fragment-->\n",
        "**Solution**: Mask out 15% of the input words, and then predict the masked words.\n",
        "> the man went to the [MASK] to buy a [MASK] of milk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNiaFKRNnwEo",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/horoshenkih/harbour-space-text-mining-course/master/pic/elmo-gpt-bert.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PB8ZzOwn-kS",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Demo: [transformer notebooks](https://huggingface.co/transformers/notebooks.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEqbdFJeoOm5",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Summary\n",
        "1. Sequence-to-Sequence (seq2seq): learn the condition with _encoder_, then generate text with conditioning with _decoder.\n",
        "1. Motivation of Transformer: learn long-range dependencies faster.\n",
        "1. Attention: measure dependency between words using dot product of vectors.\n",
        "1. Transformer block:\n",
        "  - multi-head attention + FFNN\n",
        "  - residual connections, dropout, layer normalization\n",
        "1. Encoder\n",
        "  - input: BPE, positional encoding\n",
        "1. Decoder\n",
        "  - masked self-attention, encoder-decoder attention\n",
        "1. BERT\n",
        "  - bidirectional model with masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49WrW-8GYV4m",
        "colab_type": "text"
      },
      "source": [
        "<!--@slideshow slide-->\n",
        "# Recommended resources\n",
        "- [CS224n Lecture 13: Contextual Word Representations\n",
        "and Pretraining](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture13-contextual-representations.pdf)\n",
        "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
        "- [Transformer Notebooks](https://huggingface.co/transformers/notebooks.html)\n",
        "- [Byte Pair Encoding — The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)\n",
        "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
      ]
    }
  ]
}